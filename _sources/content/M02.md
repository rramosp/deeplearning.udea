# 02 - NEURAL NETWORKS
## Redes de neuronas artificiales

**1.1 - Introducción a las redes de neuronas artificiales**: [Video 17mins](https://youtu.be/G1ymySPlh-0)<br/> Introducimos el concepto de redes de neuronas artificiales como una simplificación de su análogo biológico.

**1.2 - Entrenamiento del perceptrón**: [Video 11mins](https://youtu.be/9wD-o2mHgyM)<br/> Describimos el algoritmo de entrenamiento del perceptrón.

**1.3 - Perceptrón multicapa**: [Video 14mins](https://youtu.be/zS5WRTl8uvo)<br/> Ensamblamos una red de neuronas artificiales compuesta por varias capas de perceptrones.

**1.4 - Backpropagation**: [Video 11mins](https://youtu.be/D8W1xryHKGo)<br/>Explicamos el algoritmo principal para entrenar redes neuronales.

**1.5 - Funciones de activación**: [Video 7mins](https://youtu.be/Lq0GXDxVNP0)<br/>Introducimos las funciones de activación de neuronas y la función softmax para la salida de la red.

**1.6 - Esquemas de entrenamiento**: [Video 17mins](https://youtu.be/yFh4qIBW86s)<br/> Describimos distintas estrategias de entrenamiento: con batches, mini-batches y en línea y comprobamos experimentalmente su convergencia.

**1.7 - Sobreajuste (overfitting)**: [Video 15mins](https://youtu.be/Vekgrq60Xx4)<br/>Explicamos la intuición de los errores de predicción por sobreajuste y algunas técnicas para mitigarlo (early stopping, regularización)

**1.8 - Introducción a Keras**: [Video 22mins](https://youtu.be/KwUiDqhwTcc)<br/>Damos las nociones fundamentales del uso de Keras, la librería de alto nivel de Tensorflow para la construcción de redes neuronales.


**REFERENCIA**: [Logistic classification with cross-entropy loss](https://github.com/rramosp/2021.deeplearning/raw/main/content/local/imgs/CrossEntropy.pdf)

## LABORATORIO 1

**LAB 1 - Customized loss function**: [Video 6mins](https://youtu.be/_1nEydN1d-s)<br/>En este laboratorio crearás un modelo con Tensorflow/Keras y expandirás la función de pérdida para la optimización con regularizadores L1 y L2.

## Autoencoders

**2.1 - Introducción a autoencoders**: [Video 14mins](https://youtu.be/k24X6la0vaU) <br/>Introducimos la noción de autoencoder, como una arquitectura de red neuronal para aprendizaje no supervisado.

**2.2 - Autoencoders en Tensorflow**: [Video 9mins](https://youtu.be/OFcST3ndQ4g) <br/> Mostramos cómo implementar autoencoders con Tensroflow y cómo obtener la representación en el espacio latente de nuevos datos de entrada.

**2.3 - Interpretabilidad de autoencoders**: [Video 13mins](https://youtu.be/o9kUgnxmsfI) <br/>Ilustramos cómo podemos darle significado intuitivo a los pesos _aprendidos_ por un autoencoder.

**2.4 - Interpretabilidad de autoencoders (2)**: [Video 4mins](https://youtu.be/2W27N9iEzek) <br/>Puntualizamos un aspecto del video anterior.

**2.5 - Denoising autoencoders**: [Video 10mins](https://youtu.be/U6QHAX8cx0w) <br/>Explicamos cómo se pueden usar los autoencoders para aplicaciones de eliminación de ruido.


## LABORATORIO 2


**LAB 2 - Sparse autoencoders**: [Video 9mins](https://youtu.be/6njflcFHjW8)<br/>En este laboratorio crearás un **Sparse Autoencoder** en el que las neuronas de la representación latente adquirirán funciones más especializadas.


## Arquitecturas multimodales

**3.1 - Problemas multimodales**: [Video 11mins](https://youtu.be/shfKOfA1Cxc) <br/>Describimos el caso de uso en el que tenemos varias fuentes de información de naturaleza distinta de cada objeto de nuestro dataset (p.ej. imágenes y datos vectoriales), y queremos generar modelos que se aprovechen de esa información.

**3.2 - Arquitecturas multimodales en Tensorflow**: [Video 15mins](https://youtu.be/tBiMNVH4yF8) <br/>Explicamos cómo podemos usar el API funcional de Tensorflow para construir redes que solucionen problemas multimodales.

## LABORATORIO 3

**LAB 3 - Pair-wise image classification**: [Video 9mins](https://youtu.be/H6u5ECdNaRA)<br/>En este laboratorio montarás una arquitectura de red con múltiples entradas para una tarea de clasificación de pares de imágenes.

## The Vanishing Gradient

**4.1 - El gradiente desvaneciente**: [Video 15min](https://youtu.be/pkR-D7GwDTY) <br/> Describimos uno de los problemas que impiden que las redes neuronales se puedan entrenar.

**4.2 - Histogramas de pesos**: [Video 8mins](https://youtu.be/9HH8kpEkN8I) <br/>Mostramos cómo podemos hacerle seguimiento a la evolución de los pesos de una red durante el entrenamiento.

**4.3 - Observando el gradiente desvaneciente con TensorBoard**: [Video 15mins](https://youtu.be/jsuYeOGUJng) <br/>Desarrollamos un caso práctico donde mostramos como utilizar TensorBoard para inspeccionar la evolución de los pesos de una red durante el entrenamiento.

## Inicialización de pesos

**5.1 - Estrategias para acelerar el entrenamiento**: [Video 6mins](https://youtu.be/Gv_m-u-G7pI) <br/> Ponemos en perspectiva las distintas aproximaciones que estamos desarrollando para solventar los problemas del gradiente en el entrenamiento de las redes neuronales.

**5.2 - Inicialización de pesos**: [Video 18mins](https://youtu.be/dSsqXY_ypNQ) <br/> Explicamos la intuición y la justificación de las estrategias de inicialización de pesos.

**5.3 - Observando los efectos de la inicialización de pesos**: [Video 18mins](https://youtu.be/-PVjugMJ9No) <br/> Desarrollamos unos experimentos para poder observar en la práctica lo aprendido anteriormente.

## LABORATORIO 4

**LAB 4 - Tensorflow callbacks**: [Video 8mins](https://youtu.be/AiUBwWV3tgs)<br/>El objetivo de este laboratorio es que te familiarices con el mecanismo de callbacks de Tensorflow con el cual podrás incluir instrumentación en tu modelo para realizarle seguimiento a lo que necesites durante el entrenamiento del mismo.


