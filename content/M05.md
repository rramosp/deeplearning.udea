# 05 - SEQUENCE MODELS

## Introducción

**01 - Validación cruzada en problemas de series de tiempo**: [Video 11mins](https://youtu.be/w8xfaSksicQ)<br/> Describimos las particularidades del proceso de validación en problemas de series de tiempo, una de las aplicaciones del procesamiento de datos secuenciales.

**02 - Tareas de analítica de secuencias**: [Video 13mins](https://youtu.be/xTxLaCqUbBk)<br/>Presentamos de manera general el principio de funcionamiento de una red recurrente, los tipos de aplicaciones que se presentan en el análisis de de secuencias y las configuraciones de redes más comunes.

## Redes Neuronales Recurrentes

**03 - Introducción a las Redes Neuronales Recurrentes**: [Video 13mins](https://youtu.be/n5ropbj3lno)<br/>  Describimos los principios de funcionamiento de las RNN y la analizamos como una red densa de muchas capas.

**04 - Algoritmo de Backpropagation Through Time**: [Video 11mins](https://youtu.be/UiUSgNIvev8)<br/>  Analizamos los principios de funcionamiento del algoritmo de entrenamiento de las RNN y sus implicaciones en términos de computacionales.

**05 - Implementación de RNN en TensorFlow**: [Video 17mins](https://youtu.be/YLeoRmmYmq4)<br/> Usamos un problema de series de tiempo para implementar RNNs de una y dos capas ocultas en TensorFlow. Describimos los elementos básicos de una arquitectura Codificador-Decodificador.

## Arquitecturas recurrentes

**06 - Implementación de Arquitecturas de RNN para problemas seq-to-seq**: [Video 19mins](https://youtu.be/jitQc7YusUA)<br/>  Usamos un problema de series de tiempo en el que se desean predecir varios tiempos hacia adelante, para describir e implementar tres metodologías/arquitecturas para la solución de problemas donde tanto la entrada como la salida son secuencias.

**07 - Long Short Term Memory RNN**: [Video 22mins](https://youtu.be/jVei1bWFXMc)<br/> Presentamos los principios de funcionamiento de las redes recurrentes de tipo LSTM y GRM, así como su implementación en TensorFlow.

**08 - Truncated BPTT**: [Video 24mins](https://youtu.be/oSVbUKl2nYQ) <br/> Presentamos una variante del algoritmo de Propagación hacia atrás en el tiempo que permite realizar actualizaciones de los parámetros de la red, a partir de propagaciones parciales de una secuencia y cómo se puede realizar su implementación utilizando el framework de tensorflow.

## LABORATORIO 1

**LAB 1 - Multivariate Time-series prediction**: [Video 6mins](https://youtu.be/oK4pDy7Q1MQ) <br/>En este laboratorio debes diseñar diferentes arquitecturas de redes RNN para predecir un sólo tiempo hacia adelante y varios tiempos hacia adelante en un problema de series de tiempo multivariado.

## Procesamiento de texto

**09 - Introducción al procesamiento de texto**: [Video 17mins](https://youtu.be/IwEPJQEX-lc) <br/> Describimos las principales etapas de preprocesamiento necesarias para el uso de modelos de Machine Learning en tareas de procesamiento de lenguaje natural.

**10 - Word embeddings**: [Video 25mins](https://youtu.be/lqXdZOq9U_0) <br/> Peresentamos las razones que inspiraron la creación de técnicas de embebimiento de palabras y discutimos diversas variantes, sus principios de funcionamiento y la estrategia de transfer learning que permite usar word embeddings pre-entrenados dentro de modelos de Deep Learning en tensorflow.

**11 - Generación de secuencias usando RNNs**: [Video 9mins](https://youtu.be/VSswvuwTz-g) <br/> Describimos el procedimiento que puede ser empleado para generar secuencias artificiales a partir de redes neuronales recurrentes pre-entrenadas.

## LABORATORIO 2

**LAB 2 - Sentiment analysis in text**: [Video 10mins](https://youtu.be/tUkhHJTvE-o) <br/>En este laboratorio debes diseñar diferentes arquitecturas de DL y estrategias de transfer learning para clasificar tweets como positivos o negativos.

## Redes Neuronales Recurrentes Bidirecionales

**12 - Redes RNN Bidirecionales**: [Video 16mins](https://youtu.be/GneNfVlNq8E) <br/> Presentamos las limitaciones que tienen las redes neuronales recurrentes para procesar secuencias en las que la predicción para una posición de la secuencia, depende no sólo de las observaciones anteriores de la secuencia sino también de observaciones futuras y cómo las redes neuronales bidireccionales resuelven ese problema.

**13 - Arquitectura Encoder-Decoder con mecanismo de atención**: [Video 17mins](https://youtu.be/XsgF5bFWcew) <br/> Describimos una arquitectura particular de red recurrente de tipo codificador-decodificador, en la que se dota a la capa del decodificador de la cpacidad para seleccionar qué información de la secuencia de entrada es relevante para realizar cada una de las predicciones en la secuencia de salida.

**14 - ELMo: Embeddings from Language Models**: [Video 23mins](https://youtu.be/GC9zr2wPtZo) <br/> Describimos una arquitectura de red conocida como ELMo que permite obtener vectores de embebimiento a partir de un procesamiento de secuencias a nivel de caracter, esto permite que el vector que representa a una palabra pueda depender del contexto y no sea siempre estático.

## El modelo Transformer

**15 - Mecanismo de auto-atención**: [Video 24mins](https://youtu.be/p727fQCrw9c) <br/> Describimos el modelo conocido como Transformer y su principal principio de funcionamiento: el mecanismo de auto-atención.

**16 - Modelo BERT**: [Video 5mins](https://youtu.be/XTtcdIXskvY) <br/> Describimos el modelo BERT (Bidirectional Encoder Representations from Transformers) muy usado como modelo base para diferentes aplicaciones de NLP entre otras.

## LABORATORIO 3

**LAB 3 - Sentiment analysis with a Self-Attention Layer**: [Video 15mins](https://youtu.be/gvgjpkCAJcs) <br/>En este laboratorio debes diseñar diferentes arquitecturas de DL basadas en el modelo de Transformer para clasificar tweets como positivos o negativos.

## Arquitecturas CNN-LSTM

**17 - Arquitecturas CNN-LSTM y ConvLSTM**: [Video 22mins](https://youtu.be/deVW91RR_lQ) <br/> Presentamos las arquitecturas de redes neuronales profundas que pueden ser usadas para procesar secuencias de matrices u objetos 3D. Este tipo de arquitecturas permiten resolver tareas de ML sobre videos.

## LABORATORIO

**NON-REQUIRED LAB**

**LAB 4 - Video classification**: <br/>En este laboratorio debes diseñar arquitecturas de DL para clasificar videos de acuerdo con la acción que las personas están realizando en ellos. Este laboratorio no puede ser ejecutado en el colab, ya que la memorúa RAM disponible no es suficiente para cargar la BD que consiste de dos clases, cada una de 24 videos de 30 frames cada uno. Para quienes quieran desarrollar el ejercicio de construir una arquitectura para clasificación de videos con base en una arquitectura CNN pre-entrenada, el notebook puede ser consultado dentro del respositorio en el siguiente [enlace](https://github.com/rramosp/2021.deeplearning/blob/main/content/U5%20LAB%2004%20-%20Video%20Classification.ipynb) y debe ser ejecutado en local para lo cual deben tener instalada toda la suite necesaria de librerías y paquetes.
